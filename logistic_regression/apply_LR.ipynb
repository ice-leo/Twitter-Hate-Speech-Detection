{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c6edbc-4478-4276-87f8-bc4801143dfd",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefcc412-a4c3-4475-8d1d-4f8b685df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "from symspellpy import SymSpell\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57c1aa-fee2-4cea-83cb-fd0b40f03670",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f93953a9-9aa8-469e-8672-4dd867ff77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "with open(r'models\\logistic_model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Load the vectorizer used during training\n",
    "with open(r'models\\tfidf_vectorizer_logistic.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b418e-f364-4577-ad60-5d650dc1624b",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21538568-c51d-40b3-b3d2-b0f5ad57ea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  31963   \n",
       "1  31964   \n",
       "2  31965   \n",
       "3  31966   \n",
       "4  31967   \n",
       "\n",
       "                                                                                                                                            tweet  \n",
       "0                                                      #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦   \n",
       "1                                            @user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why    \n",
       "2                                                                         safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!   \n",
       "3  is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite  \n",
       "4                                                     3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe30c14f-5a71-47cb-8aa0-2b04741e82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17197 entries, 0 to 17196\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      17197 non-null  int64 \n",
      " 1   tweet   17197 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 268.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc6eab-f81f-4d6d-9c2f-99ee35dbe94e",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b674fc8c-e382-4a88-91bb-6c71f3a2b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Yay -- ^^\n",
    "    tweet = re.sub(r'(\\^\\^)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)    \n",
    "\n",
    "    return tweet\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    tweet = re.sub(r\"won\\'t\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"wont\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"can\\'t\", \"can not\", tweet)\n",
    "    tweet = re.sub(r\"cant\", \"can not\", tweet)\n",
    "    tweet = re.sub(r\"don\\'t\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"dont\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"didn\\'t\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"didnt\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\'t\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"wouldnt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"shouldn\\'t\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"shouldnt\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"needn\\'t\", \"need not\", tweet)\n",
    "    tweet = re.sub(r\"neednt\", \"need not\", tweet)\n",
    "    tweet = re.sub(r\"couldn\\'t\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"couldnt\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"hasn\\'t\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"hasnt\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"have\\'nt\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"was\\'nt\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"wasnt\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"were'nt\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"werent\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"isn'\\t\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"isnt\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"aren'\\t\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"arent\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"aint\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "    tweet = re.sub(r'\\bu\\b', 'you', tweet, flags=re.IGNORECASE)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('off')\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    clean_data = []\n",
    "    for i in tweet.split():\n",
    "        if i.strip() not in stop_words and i.strip().isalpha():\n",
    "            clean_data.append(i.strip())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "def shorten_consecutive(tweet):\n",
    "    tweet = re.sub(r\"(.)\\1\\1+\", r\"\\1\\1\", tweet)\n",
    "    return tweet\n",
    "\n",
    "wordStem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def stem(tweet):\n",
    "    stemmized_data = []\n",
    "    for word in tweet.split():\n",
    "        if len(word) > 1:\n",
    "            word = wordStem.stem(word)\n",
    "            stemmized_data.append(word)\n",
    "    return \" \".join(stemmized_data) \n",
    "\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda elem: re.sub(r\"(@user)\", \"\", elem))\n",
    "    df['tweet'] = df['tweet'].apply(lambda elem: re.sub(r\"(&amp\\;)\", \"\", elem))\n",
    "    df['tweet'] = df['tweet'].apply(handle_emojis)\n",
    "    df['tweet'] = df['tweet'].apply(expand_contractions)\n",
    "    df['tweet'] = df[\"tweet\"].str.replace(r\"[^A-Za-z0-9 ]+\", \"\", regex=True)\n",
    "    df['tweet'] = df['tweet'].apply(remove_stopwords) \n",
    "    df['tweet'] = df['tweet'].apply(shorten_consecutive)\n",
    "    df['tweet'] = df['tweet'].apply(stem)\n",
    "    df['tweet'].fillna('None', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500d79c5-1a3a-41e8-ad46-9cc26e576eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ba464c-effa-4e4c-bfcc-b82dbddd2e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17197 entries, 0 to 17196\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      17197 non-null  int64 \n",
      " 1   tweet   17197 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 268.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cc0de-b1c8-4ecd-a33d-64d5ce283b5e",
   "metadata": {},
   "source": [
    "# Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0595493d-ef5e-4ca6-83a2-d581e4643b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_vect = loaded_vectorizer.transform(test_df['tweet'])\n",
    "predictions = loaded_model.predict(X_new_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9262e7d-8121-42be-b6a2-d442e0caa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = test_df.copy()\n",
    "solution['label'] = predictions\n",
    "solution = solution[['id', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698bea5-82e1-4470-8bbf-2b02457d2f1f",
   "metadata": {},
   "source": [
    "# Save as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960d64ff-f4d2-4055-a4c2-28ab833d8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.to_csv('test_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
